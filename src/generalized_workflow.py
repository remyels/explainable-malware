# %%
"""
### Author: Remy El Sabeh
### Computer Specs:
#### OS: Windows Server 2016
#### Processor: Intel Xeon Gold 6140 CPU @2.30 GHz (5 processors)
#### RAM: 16GB
#### Using: Python 3.7.4

#### Library versions

##### numpy: 1.18.4
##### pandas: 0.24.2
##### shap: 0.35.0
##### lime: 0.2.0.0
##### torch: 1.5.0+cpu
##### scikit-learn: 0.23.1
##### xgboost: 0.82
##### bokeh (if it ends up being used for t-SNE or something else): 2.0.2
##### altair (if it ends up being used for t-SNE or something else): 4.1.0
##### matplotlib: 3.2.1
"""

# %%
"""
## This notebook will contain the generalized version of our workflow
### 1) Features
### 2) Classifiers
### 3) Explainers
"""

# %%
"""
### Imports
"""

# %%
import sys

import operator as op

import types

import heapq

import operator

import math

import time

import pandas as pd
import pickle
import _pickle
import numpy as np

import os 
from os import listdir
from os.path import isfile, join

# Classifiers
import torch
from torch import nn
from torch import optim

from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix
from sklearn.utils.multiclass import unique_labels
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.cluster import DBSCAN

import xgboost as xgb

# Explainers
import shap
import lime
import lime.lime_tabular

from threading import Thread
import subprocess

import re

import random

import statistics

from bokeh.models import ColumnDataSource, LabelSet
from bokeh.plotting import figure, show, output_file
from bokeh.io import output_notebook
from bokeh.layouts import gridplot

import matplotlib.pyplot as plt

from decimal import Decimal
from functools import reduce

import warnings
warnings.filterwarnings('ignore')

print("Done importing!", flush=True)

# The features used, model used and explainer used are passed as command line arguments
available_feature_sets = ['saynotooverfittingsemimodel', '2-grams']
available_models = ['rfc', 'mlr', '2layernn', 'xgboost']
available_explainers = ['shap', 'lime']

if len(sys.argv) < 4:
    print("Missing arguments!", flush=True)
    exit(1)
   

# %%
# The following three parameters are to be used to save the data to the appropriate directory. As such, their values will be specified in the corresponding functions upon the invocation of the functions
feature_dir = sys.argv[1]
model_dir = sys.argv[2]
explainer_dir = sys.argv[3]
debug = False if len(sys.argv) == 4 else (sys.argv[4] == 'debug')

if feature_dir not in available_feature_sets or model_dir not in available_models or explainer_dir not in available_explainers:
    print("One or more parameters are wrong!", flush=True)
    exit(1)

# Output the selected feature set/model/explainer of this run to stdout
print("This run consists of the following:\nFeature set: {}\nModel: {}\nExplainer: {}".format(feature_dir, model_dir, explainer_dir))

    
# Those are parameters that we need later on that are dependent on feature/model/explainer choice

parameters_of_choice = {'2-grams': {'m': 65536}, 'saynotooverfittingsemimodel': {'m': 7505}}

# %%
# This variable will store all of the parameters that go into the results, allowing us to replicate the results later on if needed
parameters = {"feature": {}, "model": {}, "explainer": {}}

# %%
# We'll use this function reccurently to avoid overwriting old data
def file_exists(path, file_name):
    return isfile(path+file_name)

# %%
# We'll use this function to print the resuls of our classification to files
# Input:
# 1) List of classifiers (in DataFrame format) (or a list of 1 element in case we want to write a single matrix consisting of all classes)
# 2) Destination file name
def write_classification_results_to_file(confusion_matrices, destination_file):
    classification_results_path = "scratch/Results/{}/{}/".format(feature_dir, model_dir)
    # Write to file
    pd.concat(confusion_matrices).to_csv(classification_results_path+destination_file)
    print("Classification results saved to file {}!".format(destination_file), flush=True)

def write_explanations_to_file(explanations, index):
    explanation_values_path = "scratch/Results/{}/{}/{}/{}".format(feature_dir, model_dir, explainer_dir, "explanation_values_{}.pickle".format(index))
    all_explanation_values = []
    for malw, expl in explanations.items():
        expl_vals_as_list = list(expl.values())
        all_explanation_values.append(expl_vals_as_list)
    pickle.dump(all_explanation_values, open(explanation_values_path, "wb"))
    print("Explanation values of hash function {} saved to file!".format(index))


# %%
"""
### The first part of our code will concern getting features based on different reduction functions
"""

# %%
"""
#### Parameters
"""

# %%
remove_question_mark_files = True if (feature_dir == '2-grams') else False
# If m/n > ngram_per_bucket_considered, considers only top k ngrams_per_bucket_considered
ngrams_per_bucket_considered = 5

parameters['feature']['question_mark_files_removed'] = remove_question_mark_files
parameters['feature']['ngrams_per_bucket_considered'] = ngrams_per_bucket_considered

# %%
# We will use this in case we are looking to debug something and we do not feel like waiting a lot for training/gathering explanations
considered_classes = [i for i in range(1, 10)] if (not debug) else [5, 9]

parameters['feature']['considered_classes'] = considered_classes

# %%
"""
#### Getting classes of the labeled malware instance
"""

# %%
labels = pd.read_csv("trainLabels.csv")
malw_to_class = {}
for row in labels.itertuples():
    if (remove_question_mark_files and row[1] in ['58kxhXouHzFd4g3rmInB', '6tfw0xSL2FNHOCJBdlaA', 'a9oIzfw03ED4lTBCt52Y', 'cf4nzsoCmudt1kwleOTI', 'd0iHC6ANYGon7myPFzBe', 'da3XhOZzQEbKVtLgMYWv', 'fRLS3aKkijp4GH0Ds6Pv', 'IidxQvXrlBkWPZAfcqKT']):
        continue
    if row[2] in considered_classes:
        malw_to_class[row[1]] = row[2]

considered_malware_names = list(malw_to_class.keys())
y = list(malw_to_class.values())

print("Total malware instances in the dataset: {}".format(len(considered_malware_names)), flush=True)

# %%
"""
# FEATURES

## Hashing is key: the next few functions are related to dimensionality reduction
"""

# %%
"""
### Helper functions
"""

# %%
def pad(ngram, length = 4):
    while (len(ngram) < length):
        ngram = '0'+ngram
    return ngram

def to_hex(integer):
    return hex(integer)[2:].upper()

def to_hex_padded(integer):
    return pad(to_hex(integer))

def to_decimal(hexval):
    return int(hexval, 16)

# %%
"""
### Considered hash functions
"""

# %%
"""
#### Dictionary-based functions

##### Input: optional seed for replicability

##### Output: dictionary mapping ngrams to metafeatures
"""

# %%
# This is a function that generates a random hash function, pass a seed if necessary
# m designates the dimension of the initial instance, n designates that of the instance that we are reducing to to
# m's value will be specified next to each feature set, and n is a value of our own choosing (defined before training)
def generate_hash_function(m, n, seed = np.random.seed(int(1000*time.time()) % 2**32)):
    if (n>m):
        raise Exception("Reduced dimensions are larger than initial dimensions")
    np.random.seed(seed)

    base_bin_size = int(m/n)

    available = {i: base_bin_size for i in range(n)}

    extras = m - base_bin_size*n

    extra_spot = list(np.random.randint(0, n, size=extras))

    for bin_index in extra_spot:
        available[bin_index] += 1

    # This is our hashing function
    index_to_metaindex = {}

    for index in range(m):
        selected_bin = np.random.choice(list(available.keys()))
        index_to_metaindex[index] = selected_bin
        available[selected_bin] -= 1
        if (available[selected_bin] == 0):
            del available[selected_bin]

    return index_to_metaindex

# %%
"""
#### Function-based functions (those are only MAD functions for now, but other functions can be included as well)

##### Input: 16-bit integer, representing initial index of a given feature
##### Output: metaindex of passed 16-bit index
"""

# %%
def mad_function_1(x):
    return (32*int(x, 16)+100) % 503

def mad_function_2(x):
    return (70* int(x, 16)+202) % 509

def mad_function_3(x):
    return (105* int(x, 16)+44) % 521

# %%
"""
## n-grams of size 2 (m = (2&ast;&ast;4)&ast;&ast;4 = 65536)

#### Input: list of malware that we are considering, function to transform those considered malware, and number of possible outcomes of transformation (NOTE: the initial representation of the malware instances is, for each malware, a dictionary mapping each ngram to its number of occurrences, though changes can be made, as long as further changes are made to the classifiers and the explainers)

#### Output: dictionary mapping from malware name to features
"""

# %%
# If the function is a dictionary
def get_feature_vector_using_dictionary(considered_malware_names, hashfunction, num_buckets, considered_classes):
    malw_to_feat = {}
    for malware_class in considered_classes:
        print("Fetching, hashing, and preprocessing malwares from class {}".format(malware_class), flush=True)
        malw_dict = pickle.load(open("scratch/family_ngrams/ngrams_{}.pickle".format(malware_class), "rb"))
        malw_dict_keys = list(malw_dict.keys())
        class_malwares = {}
        for malwarename in malw_dict_keys:
            # Those malware's byte representations are full of ?? so I'll omit those
            if malwarename in considered_malware_names:
                occ = malw_dict[malwarename]
                dictogram = {i: [(0, 'temp')]*ngrams_per_bucket_considered for i in range(num_buckets)}
                for x in occ:
                    h = hashfunction[to_decimal(x)]
                    min_heap = dictogram[h][0][0]
                    if occ[x] > min_heap:
                        heapq.heapreplace(dictogram[h], (occ[x], x))
                class_malwares[malwarename] = dictogram
            del malw_dict[malwarename]
        class_malwares_keys = list(class_malwares.keys()).copy()
        for malware in class_malwares_keys:
            top_5_occ_per_hash = []
            for hashval in class_malwares[malware]:
                top_5_occ_per_hash.append([occ for (occ, ngram) in class_malwares[malware][hashval]])
            freq_per_hash = [sum(hashval) for hashval in top_5_occ_per_hash]
            freq_per_hash = [summedhash / sum(freq_per_hash) for summedhash in freq_per_hash]
            malw_to_feat[malware] = freq_per_hash
            del class_malwares[malware]

    return malw_to_feat

# If the function is a Python function, such as a MAD function
def get_feature_vector_using_function(considered_malware_names, hashfunction, num_buckets, considered_classes):
    malw_to_feat = {}
    for malware_class in considered_classes:
        print("Fetching, hashing, and preprocessing malwares from class {}".format(malware_class), flush=True)
        malw_dict = pickle.load(open("scratch/family_ngrams/ngrams_{}.pickle".format(malware_class), "rb"))
        malw_dict_keys = list(malw_dict.keys())
        class_malwares = {}
        for malwarename in malw_dict_keys:
            # Those malware's byte representations are full of ?? so I'll omit those
            if malwarename in considered_malware_names:
                occ = malw_dict[malwarename]
                dictogram = {i: [(0, 'temp')]*ngrams_per_bucket_considered for i in range(num_buckets)}
                for x in occ:
                    h = hashfunction(x)
                    min_heap = dictogram[h][0][0]
                    if occ[x] > min_heap:
                        heapq.heapreplace(dictogram[h], (occ[x], x))
                class_malwares[malwarename] = dictogram
            del malw_dict[malwarename]
        class_malwares_keys = list(class_malwares.keys()).copy()
        for malware in class_malwares_keys:
            top_5_occ_per_hash = []
            for hashval in class_malwares[malware]:
                top_5_occ_per_hash.append([occ for (occ, ngram) in class_malwares[malware][hashval]])
            freq_per_hash = [sum(hashval) for hashval in top_5_occ_per_hash]
            freq_per_hash = [summedhash / sum(freq_per_hash) for summedhash in freq_per_hash]
            malw_to_feat[malware] = freq_per_hash
            del class_malwares[malware]
    return malw_to_feat

# %%
"""
## n-grams + assembly features (not implemented, assembly parsing already done in a separate notebook) (m = ?)
"""

# %%


# %%
"""
# Features engineered by the Kaggle's Microsoft Malware Classification Challenge (2015)'s winners (note that the features are the same across both implementations, the only difference is that some malware in the training dataset have different semi-learning labels due to the inclusion of different semi-learning models, the outputs of which were weighted (see ensemble.py))
"""

# %%
"""
## Microsoft Malware Classification Challenge (2015)'s winners' (Say no to overfitting) features (used for the generation of a single model) (m = 7505)
"""

# %%
def get_feature_vector_using_semi_model_features(considered_malware_names, considered_classes):
    df = pd.read_csv("scratch/train_semi_model_feats_after_semi_label.csv")

    # Filter by class and by name
    df = df[(df['Id'].isin(considered_malware_names)) & (df['Class'].isin(considered_classes))]

    malw_to_feats = {}

    for row in df.itertuples():
        malw_name = row[1]
        # I'm leaving out the last column as it contains the malware's class
        features = [row[i] for i in range(2, len(row)-1)]
        malw_to_feats[malw_name] = features

    # Sort everything according to the initial order, as dictated by considered_malware_names
    malw_to_feats = {malw_name:malw_to_feats[malw_name] for malw_name in considered_malware_names}

    print("Semi model features loaded for all considered malware and classes!", flush=True)

    return malw_to_feats

def get_feature_vector_using_semi_model_features_with_dictionary_reduction(considered_malware_names, hashfunction, num_buckets, considered_classes):
    # First, obtain the initial features from the other functions
    malw_to_feats = get_feature_vector_using_semi_model_features(considered_malware_names, considered_classes)

    # Then, do the necessary hashing
    for malw in considered_malware_names:
        transformed_feats = [0]*num_buckets
        for i in range(len(malw_to_feats[malw])):
            metaindex = hashfunction[i]
            transformed_feats[metaindex] += malw_to_feats[malw][i]
        del malw_to_feats[malw]
        malw_to_feats[malw] = transformed_feats

    return malw_to_feats

# %%
"""
## Microsoft Malware Classification Challenge (2015)'s winners' (Say no to overfitting) features (used for the generation of three separate models which end up getting merged) (m = ?)
"""

# %%
def get_feature_vector_using_semi_best_gen_features(considered_malware_names, considered_classes):
    features = pd.read_csv("scratch/train_semi_model_feats_after_semi_label_best_gen.csv")

    malw_to_feats = {}

    for row in features.itertuples():
        malw_name = row[1]
        # I'm leaving out the last column as it contains the malware's class
        features = [row[i] for i in range(2, len(row)-1)]
        malw_to_feats[malw_name] = features

    # Sort everything according to the initial order, as dictated by considered_malware_names
    malw_to_feats = {malw_name:malw_to_feats[malw_name] for malw_name in considered_malware_names}

    return malw_to_feats

# %%
"""
### The second part of our code will concern training classifiers on different functions
"""

# %%
"""
### This is the main workflow: we will start by defining the functions that we will use (which can be expanded) and we'll then look at the very concise piece of code that makes everything come together
"""

# %%
"""
# CLASSIFIERS
"""

# %%
"""
## Neural Network

### This function takes in malware instances and returns binary classifiers

#### Input: features and their labels

#### Output: binary classifiers
"""

# %%
def generate_classifiers_using_neural_network(X_train, y_train, X_validation, y_validation, function_index):
    confusion_matrices = []

    models = {}
    for selectedLabel in considered_classes:
        print("Working on class {}".format(selectedLabel), flush=True)

        y_train_copy = []
        for label in y_train:
            if (label == selectedLabel):
                y_train_copy.append([1.0])
            else:
                y_train_copy.append([0.0])
        X_train_copy = X_train.copy()

        model = nn.Sequential(
            nn.Linear(509, 10),
            nn.Softmax(),
            nn.Linear(10, 1),
            nn.Sigmoid()).double()

        criterion = nn.BCELoss()

        # Epochs
        epochs = 100
        print_step = epochs//10
        batch_size = 32

        # Optimizer
        optimizer = optim.SGD(model.parameters(), lr=0.01)

        for epoch in range(epochs):
            if ((epoch+1) % print_step == 0) or epoch+1 == epochs:
                print("Working on epoch number {}".format(epoch + 1), flush=True)
            permutation = torch.randperm(len(X_train_copy))

            for i in range(0,len(X_train_copy), batch_size):
                optimizer.zero_grad()

                indices = permutation[i:i+batch_size]
                temp = []
                for index in indices:
                    temp.append(X_train_copy[index])
                feat = torch.tensor(temp, dtype=torch.double)
                temp = []
                for index in indices:
                    temp.append(y_train_copy[index])
                label = torch.tensor(temp, dtype=torch.double)

                output = model(feat)
                loss = criterion(output, label)
                loss.backward()
                optimizer.step()

        # Predict for validation set
        output = model(torch.tensor(X_validation, dtype=torch.double))

        validation_labels = []

        for actual_y in y_validation:
            if actual_y == selectedLabel:
                validation_labels.append(1)
            else:
                validation_labels.append(0)

        # For each output, get the corresponding predicted class
        prediction = []
        for singleoutput in output:
            if (singleoutput > 0.5):
                prediction.append(1)
            else:
                prediction.append(0)

        # Print the confusion matrix using scikit-learn's confusion matrix (columns are predicted, rows are true)
        matrix = confusion_matrix(validation_labels, prediction)
        print("Confusion matrix: ", flush=True)
        print(matrix, flush=True)

        confusion_matrices.append(pd.DataFrame(matrix))

        # Print statistics
        print(classification_report(validation_labels, prediction), flush=True)

        models[selectedLabel] = model

    # Save the classification results to file
    write_classification_results_to_file(confusion_matrices, "binary_classifiers_{}.csv".format(function_index))

    # Let's also generate the multiclass confusion matrix using our binary classifiers
    # The outputted class will be the class with the highest outputted probability
    predicted_classes = []
    for instance in X_validation:
        outputs = []
        for selectedLabel in considered_classes:
            outputs.append(models[selectedLabel](torch.tensor(instance)))
        outputs = np.array(outputs)
        predicted_classes.append(considered_classes[np.argmax(outputs)])

    # Save the multiclass confusion matrix to file
    write_classification_results_to_file([pd.DataFrame(confusion_matrix(y_validation, predicted_classes))], "multiclass_classifier_{}.csv".format(function_index))

    return models

# %%
"""
## Microsoft Malware Classification Challenge (2015)'s winners' XGBoost classifier
### Note: the following piece of code is applicable for both sets of features from the
"""

# %%
"""
#### Parameters
"""

# %%
num_round = 200
max_depth = 2
eta = 0.25
min_child_weight = 2
colsample_bytree = 1

# %%
# Wrapper for the XGBoost classifier that makes it work for multiclass classification
class XGBC(object):
    def __init__(self, num_round = 2, max_depth = 2, eta= 1.0, min_child_weight = 2, colsample_bytree = 1, objective = 'multi:softprob'):
        self.max_depth = max_depth
        self.eta = eta
        self.colsample_bytree = colsample_bytree
        self.num_round = num_round
        self.min_child_weight = min_child_weight
        self.objective = objective
    def fit(self, train, label):
        dtrain = xgb.DMatrix(train, label = label, missing = -999)
        param = {'max_depth':self.max_depth, 'eta':self.eta,
        'colsample_bytree': self.colsample_bytree, 'min_child_weight': self.min_child_weight, 'objective':self.objective,
        'num_class':len(considered_classes)}
        self.bst = xgb.train(param, dtrain, self.num_round)
    def predict_proba(self, test):
        dtest = xgb.DMatrix(test, missing = -999)
        ypred = self.bst.predict(dtest)
        return ypred
    # Attempting to use out-of-the-box explainers for XGBoost which perform better (usually)
    def get_bst(self):
        return self.bst

# %%
def generate_classifier_using_xgboost(X_train, y_train, X_validation, y_validation, function_index):
    # Insert XGBoost parameters
    parameters['model']['num_round'] = num_round
    parameters['model']['max_depth'] = max_depth
    parameters['model']['eta'] = eta
    parameters['model']['min_child_weight'] = min_child_weight
    parameters['model']['colsample_bytree'] = colsample_bytree

    classifier = XGBC(num_round = num_round, max_depth = max_depth, eta = eta, min_child_weight = min_child_weight, colsample_bytree = colsample_bytree)

    class_to_xgboost_class = {}

    index = 0

    for label in considered_classes:
        class_to_xgboost_class[label] = index
        index += 1

    # Mapping the class names to [0, num_classes), as required by XGBoost
    y_copy = y_train.copy()
    y_copy = [class_to_xgboost_class[label] for label in y_copy]

    y_validation_copy = y_validation.copy()
    y_validation_copy = [class_to_xgboost_class[label] for label in y_validation_copy]

    X_train_copy = np.array(X_train)
    y_train_copy = np.array(y_copy)
    X_validation_copy = np.array(X_validation)
    y_validation_copy = np.array(y_validation_copy)

    print("Fitting...", flush=True)
    classifier.fit(X_train_copy, y_train_copy)

    print("Predicting...", flush=True)
    y_predicted = classifier.predict_proba(X_validation_copy)

    output = []
    for y_prediction in y_predicted:
        output.append(np.argmax(y_prediction))

    # Multiclass matrix
    matrix = confusion_matrix(y_validation_copy, output)
    print("Confusion matrix: ", flush=True)
    print(matrix, flush=True)

    # Write the multiclass matrix to file
    write_classification_results_to_file([pd.DataFrame(matrix)], "multiclass_classifier_{}.csv".format(function_index))

    # Generate multilabel confusion matrix and output all to file
    confusion_matrices = multilabel_confusion_matrix(y_validation, output, labels=[label for label in considered_classes])

    # Convert the individual matrices to DataFrame
    confusion_matrices = [pd.DataFrame(confusion_matrix) for confusion_matrix in confusion_matrices]

    write_classification_results_to_file(confusion_matrices, "binary_classifiers_{}.csv".format(function_index))

    return classifier

# %%
"""
## sklearn classifiers
"""

# %%
"""
### sklearn's Random Forest Classifier
"""

# %%
def generate_classifiers_using_sklearn_rfc(X_train, y_train, X_validation, y_validation, function_index):

    model = RandomForestClassifier().fit(X_train, y_train)

    output = model.predict(X_validation)

    # Multiclass matrix
    matrix = confusion_matrix(y_validation, output)
    print("Confusion matrix: ", flush=True)
    print(matrix, flush=True)

    # Write the multiclass matrix to file
    write_classification_results_to_file([pd.DataFrame(matrix)], "multiclass_classifier_{}.csv".format(function_index))

    # Generate multilabel confusion matrix and output all to file
    confusion_matrices = multilabel_confusion_matrix(y_validation, output, labels=[label for label in considered_classes])

    # Convert the individual matrices to DataFrame
    confusion_matrices = [pd.DataFrame(confusion_matrix) for confusion_matrix in confusion_matrices]

    write_classification_results_to_file(confusion_matrices, "binary_classifiers_{}.csv".format(function_index))

    return model


def generate_classifiers_using_sklearn_rfc_binary(X_train, y_train, X_validation, y_validation, function_index):
    confusion_matrices = []

    models = {}

    for selectedLabel in considered_classes:
        print("Working on class {}".format(selectedLabel), flush=True)

        y_train_copy = []
        for label in y_train:
            if (label == selectedLabel):
                y_train_copy.append([1.0])
            else:
                y_train_copy.append([0.0])
        X_train_copy = X_train.copy()


        model = RandomForestClassifier().fit(X_train_copy, y_train_copy)

        output = model.predict(X_validation)

        validation_labels = []

        for actual_y in y_validation:
            if actual_y == selectedLabel:
                validation_labels.append(1)
            else:
                validation_labels.append(0)

        # Binary matrix
        matrix = confusion_matrix(validation_labels, output)
        print("Confusion matrix: ", flush=True)
        print(matrix, flush=True)

        models[selectedLabel] = model
        confusion_matrices.append(matrix)

    # Save the classification results to file
    write_classification_results_to_file(confusion_matrices, "binary_classifiers_{}.csv".format(function_index))

    # Let's also generate the multiclass confusion matrix using our binary classifiers
    # The outputted class will be the class with the highest outputted probability
    predicted_classes = []
    for instance in X_validation:
        outputs = []
        for selectedLabel in considered_classes:
            outputs.append(models[selectedLabel](instance))
        outputs = np.array(outputs)
        predicted_classes.append(considered_classes[np.argmax(outputs)])

    # Save the multiclass confusion matrix to file
    write_classification_results_to_file([pd.DataFrame(confusion_matrix(y_validation, predicted_classes))], "multiclass_classifier_{}.csv".format(function_index))

    return models

# %%
"""
### sklearn's OvR LogisticRegression
"""

# %%
def generate_classifiers_using_sklearn_lr(X_train, y_train, X_validation, y_validation, function_index):
    model = LogisticRegression(multi_class="ovr").fit(X_train, y_train)

    output = model.predict(X_validation)

    # Multiclass matrix
    matrix = confusion_matrix(y_validation, output)
    print("Confusion matrix: ", flush=True)
    print(matrix, flush=True)

    # Write the multiclass matrix to file
    write_classification_results_to_file([pd.DataFrame(matrix)], "multiclass_classifier_{}.csv".format(function_index))

    # Generate multilabel confusion matrix and output all to file
    confusion_matrices = multilabel_confusion_matrix(y_validation, output, labels=[label for label in considered_classes])

    # Convert the individual matrices to DataFrame
    confusion_matrices = [pd.DataFrame(confusion_matrix) for confusion_matrix in confusion_matrices]

    write_classification_results_to_file(confusion_matrices, "binary_classifiers_{}.csv".format(function_index))

    return model

def generate_classifiers_using_sklearn_lr_binary(X_train, y_train, X_validation, y_validation, function_index):
    confusion_matrices = []

    models = {}

    for selectedLabel in considered_classes:
        print("Working on class {}".format(selectedLabel), flush=True)

        y_train_copy = []
        for label in y_train:
            if (label == selectedLabel):
                y_train_copy.append([1.0])
            else:
                y_train_copy.append([0.0])
        X_train_copy = X_train.copy()


        model = LogisticRegression(multi_class='ovr').fit(X_train_copy, y_train_copy)

        output = model.predict(X_validation)

        validation_labels = []

        for actual_y in y_validation:
            if actual_y == selectedLabel:
                validation_labels.append(1)
            else:
                validation_labels.append(0)

        # Binary matrix
        matrix = confusion_matrix(validation_labels, output)
        print("Confusion matrix: ", flush=True)
        print(matrix, flush=True)

        models[selectedLabel] = model
        confusion_matrices.append(matrix)

     # Save the classification results to file
    write_classification_results_to_file(confusion_matrices, "binary_classifiers_{}.csv".format(function_index))

    # Let's also generate the multiclass confusion matrix using our binary classifiers
    # The outputted class will be the class with the highest outputted probability
    predicted_classes = []
    for instance in X_validation:
        outputs = []
        for selectedLabel in considered_classes:
            outputs.append(models[selectedLabel](instance))
        outputs = np.array(outputs)
        predicted_classes.append(considered_classes[np.argmax(outputs)])

    # Save the multiclass confusion matrix to file
    write_classification_results_to_file([pd.DataFrame(confusion_matrix(y_validation, predicted_classes))], "multiclass_classifier_{}.csv".format(function_index))


# %%
"""
# EXPLAINERS
"""

# %%
"""
### SHAP

### The function that is passed to the KernelExplainer is used for both the sample and the indiv. malware instances from the validation set

### Also, it seems like SHAP generates explanations for each label separately...

### This function takes in binary classifiers and malware instances and returns individual malware explanation

#### Input: binary classifiers, the training set, validation malware instances and their labels

#### Output: Explanation for each malware instance
"""

# %%
"""
#### Parameters
"""

# %%
# Note: actual number generated = num_samples*background_size; num_samples is therefore basically the number of local models being trained
# If num samples is not set, the number of evaluated models is equal to 2*instance_dim + 2048
shap_num_samples = 500
background_size = 50
number_of_shap_batches = 10

# %%
"""
#### We have different SHAP functions, each of which treats the problem slightly differently (the last 2 functions are not tested yet, but minor corrections are needed (if any))
"""

# %%
def generate_batch_job(i, features, model, explainer):
    subprocess.call("python explainer_batch_generator_runner.py {} {} {} {}".format(i, features, model, explainer), shell=True)

def generate_shap_explanations_normalized(classifiers, X_train, X_validation, malw_names_in_validation):
    parameters['explainer']['shap_num_samples'] = shap_num_samples
    parameters['explainer']['background_size'] = background_size

    np_X_validation = np.array(X_validation)

    print("The whole size of the sample we are trying to get the explanation for is {}".format(np_X_validation.shape[0]), flush=True)
    # Get the batch size as the floor of the total number of malware instances divided by the number of batches-1
    # The remaining malware instances, if any, will constitude the last batch
    batch_size = np_X_validation.shape[0]//(number_of_shap_batches-1)
    remaining = np_X_validation.shape[0]-(batch_size*(number_of_shap_batches-1))

    print("We have {} batches of {} instances each and 1 batch of {}".format(number_of_shap_batches-1, batch_size, remaining), flush=True)

    data = []

    # shap.sample uses sklearn.utils.resample
    # explainer = shap.KernelExplainer(shap_model_output, shap.sample(np.array(X_train), background_size))

    # pickle.dump(shap_model_output, open("shap_model_output.pickle", "wb"))
    pickle.dump(shap.sample(np.array(X_train), background_size), open("{}_{}_{}_shap_sample.pickle".format(feature_dir, model_dir, explainer_dir), "wb"))
    pickle.dump(classifiers, open("{}_{}_{}_classifiers.pickle".format(feature_dir, model_dir, explainer_dir), "wb"))
    pickle.dump(considered_classes, open("{}_{}_{}_considered_classes.pickle".format(feature_dir, model_dir, explainer_dir), "wb"))

    for i in range(0, batch_size*(number_of_shap_batches-1), batch_size):
        batch = np_X_validation[i:i+batch_size]
        data.append(batch)
        pickle.dump(data, open("{}_{}_{}_batch_{}".format(feature_dir, model_dir, explainer_dir, i//batch_size+1), 'wb'))
        print("Batch number {} created!".format(i//batch_size+1), flush=True)
        data = []

    # Let's not forget the last batch!
    data.append(np_X_validation[-remaining:])
    pickle.dump(data, open("{}_{}_{}_batch_{}".format(feature_dir, model_dir, explainer_dir, number_of_shap_batches), 'wb'))
    print("Batch number {} created!".format(number_of_shap_batches), flush=True)

    threads = []

    # Now, we create the jobs
    for i in range(1, number_of_shap_batches+1):
        threads.append(Thread(target=generate_batch_job, args=(i,feature_dir, model_dir, explainer_dir)))
        print("Created explanation batch job {}".format(i), flush=True)

    counter = 1

    # Launch the jobs
    for t in threads:
        t.start()
        print("Launched explanation batch job number {}!".format(counter), flush=True)
        counter += 1

    counter = 1

    # Wait for all the jobs to complete
    for t in threads:
        t.join()
        print("Explanation batch job numer {} ended!".format(counter), flush=True)
        counter += 1

    # Each batch will return a list of lists (each list is for a label and each list is of length # of elements, we'll combine the batches together)
    temp_shap_values = []

    # Finally, get all the results back in one dictionary
    for i in range(1, number_of_shap_batches+1):
        result_file_name = "{}_{}_{}_result_batch_{}".format(feature_dir, model_dir, explainer_dir, i)
        # Busy waiting for the files to be written
        while not os.path.exists(result_file_name):
            time.sleep(1)
        batch_result = pickle.load(open(result_file_name, "rb"))
        for malw_explanation in batch_result:
            temp_shap_values.append(malw_explanation)
        print("Obtained results from batch {}".format(i), flush=True)            

    # Remove all the files that we just created
    result_files = "{}_{}_{}_result_batch_".format(feature_dir, model_dir, explainer_dir)
    batch_files = "{}_{}_{}_batch_".format(feature_dir, model_dir, explainer_dir)
    os.system("rm {}*".format(result_files))
    os.system("rm {}*".format(batch_files))
    print("Removed the shell scripts, the data and the batch results", flush=True)


    shap_values = []

    for i in range(len(considered_classes)):
        temp = []
        for j in range(i, len(temp_shap_values), len(considered_classes)):
            for k in temp_shap_values[j]:
                temp.append(k)
        shap_values.append(temp)
        print("Done combining result of batches for class {}".format(considered_classes[i]))


    # This will correspond to the desired output
    malw_to_shap_values = {}

    for j in range(len(malw_names_in_validation)):
        malw_name = malw_names_in_validation[j]
        # NOTE: explainer.shap_values returns explanations for each label: in the 2nd and the third implementation of
        # the model function, we only have one label, so we do not have to worry about that
        # In the multiclass case, explanation is linked to classifier of malware class here
        list_of_shap_values = list(shap_values[considered_classes.index(malw_to_class[malw_names_in_validation[j]])][j])
        # Convert list_of_shap_values to feature_to_shap_value for it to fit the format
        feature_to_shap_value = {k: v for k, v in enumerate(list(list_of_shap_values))}
        # Sort the metafeatures by decreasing abs(score)
        feature_to_shap_value = {k: feature_to_shap_value[k] for k in sorted(feature_to_shap_value, key=lambda dict_key: abs(feature_to_shap_value[dict_key]), reverse=True)}
        # Remove metafeature with value = 0
        feature_to_shap_value = {k:v for k,v in feature_to_shap_value.items() if v != 0}
        malw_to_shap_values[malw_name] = feature_to_shap_value

    return malw_to_shap_values

def generate_shap_explanations_binary_continuous(classifiers, X_train, X_validation, malw_names_in_validation):
    parameters['explainer']['shap_num_samples'] = shap_num_samples
    parameters['explainer']['background_size'] = background_size

    # Eventually, the results will be stored here for every malware instance in the validation set
    malw_to_shap_values = {}

    for considered_class in considered_classes:
        print("Working on gathering the explanations of malware from the validation set with label {}".format(considered_class), flush=True)
        # First, select the required binary classifier
        classifier_required = classifiers[considered_class]

        # Then, get the malware that have the label corresponding to the label that the classifier is trying to predict
        malw_names_in_validation_in_class = []
        np_X_validation = []

        for i in range(len(malw_names_in_validation)):
            malw_name = malw_names_in_validation[i]
            if (malw_to_class[malw_name] == considered_class):
                malw_names_in_validation_in_class.append(malw_name)
                np_X_validation.append(X_validation[i])

        np_X_validation = np.array(np_X_validation)

        # Returns the probability of the instance belonging to its actual class
        # Unlike the previous function, this one is more sensitive to small changes in probability
        # as there is no "cutoff" here
        def shap_model_output_binary_probability(data):
                predictions = []
                for perturbation in data:
                    predictions.append(float(classifier_required(torch.from_numpy(perturbation))[0]))
                return np.array(predictions)

        # Then, we initialize the explainer
        print("Initializing KernelExplainer", flush=True)

        # Converting to list and back to nparray
        X_train = [list(x) for x in X_train]
        X_train = np.array(X_train)

        # shap.sample uses sklearn.utils.resample
        explainer = shap.KernelExplainer(shap_model_output_binary_probability, shap.sample(X_train, background_size))

        # Then, we explain the concerned malware instances
        print("Now we explain individual malware instances", flush=True)

        shap_values = explainer.shap_values(np_X_validation, nsamples=shap_num_samples)

        for j in range(len(malw_names_in_validation_in_class)):
            malw_name = malw_names_in_validation_in_class[j]
            list_of_shap_values = list(shap_values[j])
            # Convert list_of_shap_values to feature_to_shap_value for it to fit the format
            feature_to_shap_value = {k: v for k, v in enumerate(list(list_of_shap_values))}
            # Sort the metafeatures by decreasing abs(score)
            feature_to_shap_value = {k: feature_to_shap_value[k] for k in sorted(feature_to_shap_value, key=lambda dict_key: abs(feature_to_shap_value[dict_key]), reverse=True)}
            # Remove metafeature with value = 0
            feature_to_shap_value = {k:v for k,v in feature_to_shap_value.items() if v != 0}
            malw_to_shap_values[malw_name] = feature_to_shap_value

    return malw_to_shap_values

def generate_shap_explanations_for_sklearn_tree_models(model, X_train, X_validation, malw_names_in_validation):
#     parameters['explainer']['shap_num_samples'] = shap_num_samples
    parameters['explainer']['background_size'] = background_size


     # Eventually, the results will be stored here for every malware instance in the validation set
    malw_to_shap_values = {}

    validation_features = np.array(X_validation)

    explainer = shap.TreeExplainer(model, data = shap.sample(np.array(X_train), background_size))
    # Since this returns a list, need to map the labels to indices (still need to verify if the sorting is correct)
    shap_values = explainer.shap_values(validation_features, check_additivity=False)

    class_to_shap_values_index = {}

    index = 0

    for label in considered_classes:
        class_to_shap_values_index[label] = index
        index += 1

    # We are only interested in the explanations of the class that corresponds to the malware's actual class

    for i in range(len(malw_names_in_validation)):
        malw_name = malw_names_in_validation[i]
        shap_class_index = class_to_shap_values_index[malw_to_class[malw_name]]
        feature_to_shap_value = {k: v for k, v in enumerate(list(shap_values[shap_class_index][i]))}
        # Sort the metafeatures by decreasing abs(score)
        feature_to_shap_value = {k: feature_to_shap_value[k] for k in sorted(feature_to_shap_value, key=lambda dict_key: abs(feature_to_shap_value[dict_key]), reverse=True)}
        # Remove metafeature with value = 0
        feature_to_shap_value = {k:v for k,v in feature_to_shap_value.items() if v != 0}
        malw_to_shap_values[malw_name] = feature_to_shap_value

    return malw_to_shap_values

def generate_shap_explanations_for_sklearn_tree_models_binary_models(models, X_train, X_validation, malw_names_in_validation):
#     parameters['explainer']['shap_num_samples'] = shap_num_samples
    parameters['explainer']['background_size'] = background_size


     # Eventually, the results will be stored here for every malware instance in the validation set
    malw_to_shap_values = {}

    index = 0

    for selected_label in considered_classes:
        selected_feats = []
        selected_malware_names = []
        for i in range(len(malw_names_in_validation)):
            if malw_to_class[malw_names_in_validation[i]] == selected_label:
                selected_feats.append(X_validation[i])
                selected_malware_names.append(malw_names_in_validation[i])
        
        validation_features = np.array(selected_feats)

        explainer = shap.TreeExplainer(models[index], data = shap.sample(np.array(X_train), background_size))
        # Since this returns a list, need to map the labels to indices (still need to verify if the sorting is correct)
        shap_values = explainer.shap_values(validation_features, check_additivity=False)

        # We are only interested in the explanations of the class that corresponds to the malware's actual class

        for i in range(len(selected_malware_names)):
            malw_name = selected_malware_names[i]
            # 1 means we are only interested in the explanations for the malware belonging to the class
            feature_to_shap_value = {k: v for k, v in enumerate(list(shap_values[1][i]))}
            # Sort the metafeatures by decreasing abs(score)
            feature_to_shap_value = {k: feature_to_shap_value[k] for k in sorted(feature_to_shap_value, key=lambda dict_key: abs(feature_to_shap_value[dict_key]), reverse=True)}
            # Remove metafeature with value = 0
            feature_to_shap_value = {k:v for k,v in feature_to_shap_value.items() if v != 0}
            malw_to_shap_values[malw_name] = feature_to_shap_value

        index += 1

    return malw_to_shap_values


def generate_shap_explanations_for_sklearn_lr(model, X_train, X_validation, malw_names_in_validation):
    parameters['explainer']['shap_num_samples'] = shap_num_samples
    parameters['explainer']['background_size'] = background_size

    # Eventually, the results will be stored here for every malware instance in the validation set
    malw_to_shap_values = {}

    # Convert labels to indices
    class_to_shap_values_index = {}

    index = 0

    for label in considered_classes:
        class_to_shap_values_index[label] = index
        index += 1

    for considered_class in considered_classes:
        print("Working on gathering the explanations of malware from the validation set with label {}".format(considered_class), flush=True)

        # Then, get the malware that have the label corresponding to the label that the classifier is trying to predict
        malw_names_in_validation_in_class = []
        np_X_validation = []

        for i in range(len(malw_names_in_validation)):
            malw_name = malw_names_in_validation[i]
            if (malw_to_class[malw_name] == considered_class):
                malw_names_in_validation_in_class.append(malw_name)
                np_X_validation.append(X_validation[i])

        np_X_validation = np.array(np_X_validation)

        # Returns the probability of the instance belonging to its actual class
        def shap_model_output(array):
                outputs = model.predict_proba(array)
                prediction = []
                for output in outputs:
                    prediction.append(output)
                return np.array(prediction)

        # Then, we initialize the explainer
        print("Initializing KernelExplainer", flush=True)

        # shap.sample uses sklearn.utils.resample
        explainer = shap.KernelExplainer(shap_model_output, shap.sample(np.array(X_train), background_size))

        # Then, we explain the concerned malware instances
        print("Now we explain individual malware instances", flush=True)

        shap_values = explainer.shap_values(np_X_validation, nsamples=shap_num_samples)

        for j in range(len(malw_names_in_validation_in_class)):
            malw_name = malw_names_in_validation_in_class[j]
            list_of_shap_values = list(shap_values[selected_label][j])
            # Convert list_of_shap_values to feature_to_shap_value for it to fit the format
            feature_to_shap_value = {k: v for k, v in enumerate(list(list_of_shap_values))}
            # Sort the metafeatures by decreasing abs(score)
            feature_to_shap_value = {k: feature_to_shap_value[k] for k in sorted(feature_to_shap_value, key=lambda dict_key: abs(feature_to_shap_value[dict_key]), reverse=True)}
            # Remove metafeature with value = 0
            feature_to_shap_value = {k:v for k,v in feature_to_shap_value.items() if v != 0}
            malw_to_shap_values[malw_name] = feature_to_shap_value

    return malw_to_shap_values

def generate_shap_explanations_for_sklearn_lr_binary_models(models, X_train, X_validation, malw_names_in_validation):
    parameters['explainer']['shap_num_samples'] = shap_num_samples
    parameters['explainer']['background_size'] = background_size

    # Eventually, the results will be stored here for every malware instance in the validation set
    malw_to_shap_values = {}

    index = 0

    for considered_class in considered_classes:
        print("Working on gathering the explanations of malware from the validation set with label {}".format(considered_class), flush=True)

        # Then, get the malware that have the label corresponding to the label that the classifier is trying to predict
        malw_names_in_validation_in_class = []
        np_X_validation = []

        for i in range(len(malw_names_in_validation)):
            malw_name = malw_names_in_validation[i]
            if (malw_to_class[malw_name] == considered_class):
                malw_names_in_validation_in_class.append(malw_name)
                np_X_validation.append(X_validation[i])

        np_X_validation = np.array(np_X_validation)

        # Returns the probability of the instance belonging to its actual class
        def shap_model_output(array):
                outputs = models[index].predict_proba(array)
                prediction = []
                for output in outputs:
                    prediction.append(output[1])
                return np.array(prediction)

        # Then, we initialize the explainer
        print("Initializing KernelExplainer", flush=True)

        # shap.sample uses sklearn.utils.resample
        explainer = shap.KernelExplainer(shap_model_output, shap.sample(np.array(X_train), background_size))

        # Then, we explain the concerned malware instances
        print("Now we explain individual malware instances", flush=True)

        shap_values = explainer.shap_values(np_X_validation, nsamples=shap_num_samples)

        for j in range(len(malw_names_in_validation_in_class)):
            malw_name = malw_names_in_validation_in_class[j]
            list_of_shap_values = list(shap_values[j])
            # Convert list_of_shap_values to feature_to_shap_value for it to fit the format
            feature_to_shap_value = {k: v for k, v in enumerate(list(list_of_shap_values))}
            # Sort the metafeatures by decreasing abs(score)
            feature_to_shap_value = {k: feature_to_shap_value[k] for k in sorted(feature_to_shap_value, key=lambda dict_key: abs(feature_to_shap_value[dict_key]), reverse=True)}
            # Remove metafeature with value = 0
            feature_to_shap_value = {k:v for k,v in feature_to_shap_value.items() if v != 0}
            malw_to_shap_values[malw_name] = feature_to_shap_value

        index += 1

    return malw_to_shap_values


# %%
"""
### LIME


#### Small note regarding LIME: By default, the explanations of LIME return the features with the biggest scores in absolute value. If we are looking to only use negative or positive scores, toggle only_positive below

#### Input: binary classifiers, the set of malware instances, validation malware instances and their labels

#### Output: Explanation for each malware instance
"""

# %%
"""
#### Parameters
"""

# %%
# Denotes the generated number of samples per malware (Note: default is 5000)
number_of_lime_samples = 1000
# Denotes if we should only preserve explanations with positive scores
only_positive = False

# %%
"""
### LIME Tabular Explainer works with any classifier (not just the scikit-learn implementations!)
"""

# %%
"""
#### Parameter: selected prediction function of choice can be found in the function definition below (cannot be put outside as the function it is equal to has to be in the scope of the generate_lime_explanation)
"""

# %%
"""
#### Eventually refer back to what's explained in this link: https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html (to double-check that everything is OK)

#### Also, the below code is not guaranteed to work for anything other than 2-grams, so recheck with other features
"""

# %%
def generate_lime_explanations(classifiers, X_train, X_validation, malw_names_in_validation):
    parameters['explainer']['number_of_lime_samples'] = number_of_lime_samples
    parameters['explainer']['only_positive'] = only_positive

    # The beginning of this function is dedicated for prediction functions

    # This function takes in the perturbed data
    def lime_model_output_torch(data):
        predictions = []
        for sample in data:
            prediction = []
            for class_ in considered_classes:
                prediction.append(float(classifiers[class_](torch.from_numpy(sample))[0]))
            prediction = [x/sum(prediction) for x in prediction]
            predictions.append(prediction)
        return np.array(predictions)

    # Similar to the function above, except that it is supposed to work for saynotooverfitting's code
    # NOT FIXED YET
    def lime_model_output_xgboost(data):
        return np.array(classifiers.predict_proba(data))

    selected_function = lime_model_output_torch if (model_dir == '2layernn') else lime_model_output_xgboost

    feature_names = ["hashval{}".format(i) for i in range(len(X_train[0]))]

    explainer = lime.lime_tabular.LimeTabularExplainer(np.array(X_train), feature_names = feature_names, class_names = [i for i in considered_classes])


    malw_to_explanation = {}

    for i in range(len(X_validation)):
        if ((i+1)%50 == 0 or i+1 == len(X_validation)):
            print("LIME: Explaining instance {}/{}".format(i+1, len(X_validation)), flush=True)

        malw_name = malw_names_in_validation[i]

        # labels = malw_class means that we are generating explanations for the specified label, which is the class of the malware
        exp = explainer.explain_instance(np.array(X_validation[i]), selected_function, num_features=len(X_validation[i]), labels = [considered_classes.index(malw_to_class[malw_name])], num_samples=number_of_lime_samples)

        explanation_adjusted = {}

        # Example of how this looks like: [('hashval45 <= -0.18', -0.004488891263510804), ('hashval342 > -0.17', -0.003889412522256591), ('hashval292 <= -0.39', -0.0036020763927755283), ('hashval385 > 0.49', 0.00304904659907303), ('hashval44 <= -0.35', 0.00285650138147016), ('hashval67 > 0.14', 0.00269333741266697), ('hashval299 > -0.05', -0.0026852358911554895)...]
        # It looks like this gives a score for all features, unlike SHAP
        explanation_of_malware = exp.as_list(considered_classes.index(malw_to_class[malw_name]))

        # Remove negatively scoring features if needed
        if only_positive:
            explanation_of_malware = [x for x in explanation_of_malware if x[1] > 0]

        for feat in explanation_of_malware:
            # To get the concerned feature, we should match hashval\d+, look at how the explanation looks like above to understand
            feat_num = re.search("hashval\d+", feat[0]).group(0)[7:]
            score = feat[1]
            explanation_adjusted[int(feat_num)] = score

        malw_to_explanation[malw_name] = explanation_adjusted

    return malw_to_explanation


def generate_lime_explanations_rfc(classifier, X_train, X_validation, malw_names_in_validation):
    parameters['explainer']['number_of_lime_samples'] = number_of_lime_samples
    parameters['explainer']['only_positive'] = only_positive

    selected_function = classifier.predict_proba

    feature_names = ["hashval{}".format(i) for i in range(len(X_train[0]))]

    explainer = lime.lime_tabular.LimeTabularExplainer(np.array(X_train), feature_names = feature_names, class_names = [i for i in considered_classes])


    malw_to_explanation = {}

    for i in range(len(X_validation)):
        if ((i+1)%50 == 0 or i+1 == len(X_validation)):
            print("LIME: Explaining instance {}/{}".format(i+1, len(X_validation)), flush=True)

        malw_name = malw_names_in_validation[i]

        # labels = malw_class means that we are generating explanations for the specified label, which is the class of the malware
        exp = explainer.explain_instance(np.array(X_validation[i]), selected_function, num_features=len(X_validation[i]), labels = [considered_classes.index(malw_to_class[malw_name])], num_samples=number_of_lime_samples)

        explanation_adjusted = {}

        # Example of how this looks like: [('hashval45 <= -0.18', -0.004488891263510804), ('hashval342 > -0.17', -0.003889412522256591), ('hashval292 <= -0.39', -0.0036020763927755283), ('hashval385 > 0.49', 0.00304904659907303), ('hashval44 <= -0.35', 0.00285650138147016), ('hashval67 > 0.14', 0.00269333741266697), ('hashval299 > -0.05', -0.0026852358911554895)...]
        # It looks like this gives a score for all features, unlike SHAP
        explanation_of_malware = exp.as_list(considered_classes.index(malw_to_class[malw_name]))

        # Remove negatively scoring features if needed
        if only_positive:
            explanation_of_malware = [x for x in explanation_of_malware if x[1] > 0]

        for feat in explanation_of_malware:
            # To get the concerned feature, we should match hashval\d+, look at how the explanation looks like above to understand
            feat_num = re.search("hashval\d+", feat[0]).group(0)[7:]
            score = feat[1]
            explanation_adjusted[int(feat_num)] = score

        malw_to_explanation[malw_name] = explanation_adjusted

    return malw_to_explanation

# %%
"""
### We conclude our function definition with the above. Now, we run the code that makes everything come together
"""

# %%
"""
#### Parameters
"""

# %%
number_of_functions = 3

# %%
"""
#### Global variables
"""

# %%
# This will let us know whether scaling features is necessary (usually redundant for decision tree based models)
scale_features = False if (model_dir == 'rfc') else True

# This is the initial dimension of the instances in our dataset (depends on the picked feature set)
m = parameters_of_choice[feature_dir]['m']

# This is the dimension that we are reducing to
n = 509

parameters['feature']['number_of_functions'] = number_of_functions
parameters['feature']['scale_features'] = scale_features
parameters['feature']['m'] = m
parameters['feature']['n'] = n

# %%
# Either use dictionary-based hash functions
list_of_functions = [generate_hash_function(m, n, seed = i) for i in range(number_of_functions)]
# Or function-based hash functions
# list_of_functions = []

# This is where we will be storing the explanations for each function
function_index_to_explanations = {}

# Auxiliary storage, will be dumped to file later to save results
list_of_models = [None]*len(list_of_functions)
list_of_explanations = [None]*len(list_of_functions)

# Those are the malware instances that did not contribute towards the training of our classifiers
# They will be used for validation as well as for explanation gathering
malw_names_in_validation = []

for f in range(len(list_of_functions)):
    # Depending on whether the functions used are dictionary-based or function-based, use the appropriate function to get feature vector
    if (feature_dir == '2-grams'):
        features = get_feature_vector_using_dictionary(considered_malware_names, list_of_functions[f], n, considered_classes)
    else:
        features = get_feature_vector_using_semi_model_features_with_dictionary_reduction(considered_malware_names, list_of_functions[f], n, considered_classes)
    
    features = list(features.values())

    if scale_features:
        features = StandardScaler().fit_transform(features)

    # We'll need this to preserve the mapping from malware name to features for explanations
    X = tuple(zip(y, considered_malware_names, features))

    # First split: X_validation and y_validation will only be used in our explainers to generate explanations
    # While the features themselves do change, the split in terms of element indices don't, so the malware that
    # are neither trained or tested on (i.e. the ones we use to generate explanations) will not change across
    # hash functions!
    X_train, X_validation, y_train, y_validation = train_test_split(X, y,
                                                    stratify=y,
                                                    test_size=0.20,
                                                    random_state=42)

    X_train = [feats for (label, name, feats) in X_train]

    # Same malware names in validation selected due to how train_test_split works for the same random state,
    # no matter the iteration, so just save once
    if (len(malw_names_in_validation) == 0):
        malw_names_in_validation = [name for (label, name, feats) in X_validation]
    X_validation = [feats for (label, name, feats) in X_validation]

    # Generates model using the method of choice
    if (model_dir == '2layernn'):
        model = generate_classifiers_using_neural_network(X_train, y_train, X_validation, y_validation, f)
    elif (model_dir == 'rfc'):
        model = generate_classifiers_using_sklearn_rfc(X_train, y_train, X_validation, y_validation, f)
    elif (model_dir == 'mlr'):
        model = generate_classifiers_using_sklearn_lr(X_train, y_train, X_validation, y_validation, f)
    else:
        model = generate_classifier_using_xgboost(X_train, y_train, X_validation, y_validation, f)

    list_of_models[f] = model

    # Generates explanations for each malware using the method of choice
    # NOTE: XGBoost only works with lime so far
    if (explainer_dir == 'shap'):
        if (model_dir == '2layernn' or model_dir == 'xgboost'):
            explanations = generate_shap_explanations_normalized(model, X_train, X_validation, malw_names_in_validation)
            # explanations = generate_shap_explanations_binary_continuous(model, X_train, X_validation, malw_names_in_validation)
            # explanations = generate_shap_explanations_binary_discrete(model, X_train, X_validation, malw_names_in_validation)
        elif (model_dir == 'rfc'):
            explanations = generate_shap_explanations_for_sklearn_tree_models(model, X_train, X_validation, malw_names_in_validation)
        elif (model_dir == 'mlr'):
            explanations = generate_shap_explanations_for_sklearn_lr(model, X_train, X_validation, malw_names_in_validation)    
    else:
        if (model_dir == '2layernn' or model_dir == 'xgboost'):
            explanations = generate_lime_explanations(model, X_train, X_validation, malw_names_in_validation)
        else:
            explanations = generate_lime_explanations_rfc(model, X_train, X_validation, malw_names_in_validation)

    function_index_to_explanations[f] = explanations

# %%
"""
## We have arrived to the last part of the analysis. From here on out, we will be checking our explanations across the functions
"""

# %%
"""
### Parameters
"""

# %%
# In case the explanations are based on a score, you can pick what number of explanations you want to select
# starting from the top, by setting this variable to a positive number
explanation_consideration_cutoff = 20

parameters['explainer']['explanation_consideration_cutoff'] = explanation_consideration_cutoff

# %%
"""
### First, filter explanations if needed (in case explanation_consideration_cutoff > 0)
"""

# %%
print("Overall number of malware instances we will be looking at the explanations of: {}".format(len(malw_names_in_validation)), flush=True)

malw_to_per_function_metafeature_explanation = {}

for i in range(len(malw_names_in_validation)):
    consider_malware = True
    malw_name = malw_names_in_validation[i]
    per_function_metafeature_explanation = []
    malw_class = malw_to_class[malw_name]
    for function_index in range(len(list_of_functions)):
        # All explainer functions return explanations as a dictionary (index -> explanation score) sorted by decreasing score
        metafeature_explanation = function_index_to_explanations[function_index][malw_name]
        # Convert the dictionary to a list
        metafeature_explanation = [x[0] for x in metafeature_explanation.items()]
        # We can consider only the top X explanations per malware to make our study a bit more homogeneous
        if explanation_consideration_cutoff > 0:
            metafeature_explanation = metafeature_explanation[:explanation_consideration_cutoff]
        per_function_metafeature_explanation.append([x for x in metafeature_explanation])
    if (consider_malware):
        malw_to_per_function_metafeature_explanation[malw_name] = per_function_metafeature_explanation
                                                                               
print("Considered: {}".format(len(malw_to_per_function_metafeature_explanation)), flush=True)

# %%
"""
### Then, for each malware instance, look at what initial explanations were preserved across functions, and get the intersection of those explanations across functions
"""

# %%
# Stores the length of the intersection across functions for each malware
malw_to_len_inters = {}
# Stores the common ngram explanations across functions for each malware
malw_to_inters = {}

for selected_malware in malw_to_per_function_metafeature_explanation:
    # List that stores the ngram explanations for each function
    considered_features_per_function = []
    for i in range(len(list_of_functions)):
        considered_ngrams = []
        for j in range(m):
            # Depending on whether the functions used are dictionary-based or function-based, use the appropriate line
            if (list_of_functions[i][j] in malw_to_per_function_metafeature_explanation[selected_malware][i]):
#             if (list_of_functions[i](i) in malw_to_per_function_metafeature_explanation[selected_malware][i]):
                considered_ngrams.append(j)
        considered_features_per_function.append(considered_ngrams)

    # Now, we get the intersection of the lists in considered_features_per_function
    intersection_of_considered_ngrams_across_functions = set(considered_features_per_function[0])
    for i in range(1, len(list_of_functions)):
        intersection_of_considered_ngrams_across_functions = set([x for x in considered_features_per_function[i] if x in intersection_of_considered_ngrams_across_functions])
    malw_to_len_inters[selected_malware] = len(intersection_of_considered_ngrams_across_functions)
    malw_to_inters[selected_malware] = list(intersection_of_considered_ngrams_across_functions)

# Sorting (probably irrelevant and is just used in case we want to print and look at the results)
malw_to_len_inters = {malwarename: len_inters for malwarename, len_inters in sorted(malw_to_len_inters.items(),key=operator.itemgetter(1),reverse=True)}
malw_to_inters = {malwarename: inters for malwarename, inters in sorted(malw_to_inters.items(),key=operator.itemgetter(1), reverse=True)}

# %%
"""
### This part will allow us to examine the performance of the dimension-reducing function paired with the explanation framework that we're using
"""

# %%
total_considered = len(malw_to_len_inters)

# %%
"""
#### We construct a dictionary mapping len_inters to number of malware instances, we'll use the intersection length on our x-axis and the number of malware instances corresponding to the intersection length on our y-axis
"""

# %%
len_inters_to_num_malware = {}
for v in malw_to_len_inters.values():
    if v in len_inters_to_num_malware:
        len_inters_to_num_malware[v] += 1
    else:
        len_inters_to_num_malware[v] = 1

# For the plot to work properly, we need to fill in the "gaps", i.e. the intersection lengths with no corresponding malware
max_len_inters = max(len_inters_to_num_malware.keys())

for len_inters in range(max_len_inters):
    if len_inters not in len_inters_to_num_malware:
        len_inters_to_num_malware[len_inters] = 0

# Sort by len_inters
len_inters_to_num_malware = dict(sorted(len_inters_to_num_malware.items()))

# %%
print(len_inters_to_num_malware, flush=True)


# %%
"""
## To conclude, what we want to do is to group the intersection explanations by class and then look at similarities between malware of the same class
"""

# %%
"""
#### First, we create the corresponding dictionary
"""

# %%
class_to_malware_to_explanation = {}
for malw, malw_expl in malw_to_inters.items():
    malw_class = malw_to_class[malw]
    if malw_class in class_to_malware_to_explanation:
        class_to_malware_to_explanation[malw_class][malw] = malw_expl
    else:
        class_to_malware_to_explanation[malw_class] = {malw: malw_expl}

# Print intersection of explanations per class
for malw_class in considered_classes:
    malw_expl = class_to_malware_to_explanation[malw_class]
    print("Working on class number {}".format(malw_class), flush=True)
    print([x[1] for x in malw_expl.items()], flush=True)

# %%
"""
#### Then, we check frequencies of initial explanations within the same class
"""

# %%
class_to_explanation_occ = {}

for malw_class in considered_classes:
    print("Working on class {} with {} malware instances in validation".format(malw_class, len([malw for malw in malw_to_inters if malw_to_class[malw]==malw_class])), flush=True)
    explanation_to_occ = {}
    for malware, explanations in class_to_malware_to_explanation[malw_class].items():
        for explanation in explanations:
            if (explanation in explanation_to_occ):
                explanation_to_occ[explanation] +=1
            else:
                explanation_to_occ[explanation] = 1
    sorted_occ = {explanation: occ for explanation, occ in sorted(explanation_to_occ.items(),key=operator.itemgetter(1), reverse=True)}
    class_to_explanation_occ[malw_class] = sorted_occ
    print(sorted_occ, flush=True)

# %%
"""
#### Save the data to files for futher inspection
"""

# %%
"""
#### NOTE: This code was implemented in a way that halts execution in case the file we are trying to write already exists. If this is the desired behavior, clear the content of the directory first, then rerun
"""

# %%
path = "scratch/Results/{}/{}/{}/".format(feature_dir, model_dir, explainer_dir)
model_file = "models.pickle"
explanations_file = "explanations.pickle"
intersection_explanations_file = "intersection_explanations.pickle"
class_explanations_file = "class_explanations.pickle"
class_explanations_occurrence_file = "class_explanations_occ.pickle"
parameters_file = "parameters.pickle"

# %%
print(path, flush=True)

if file_exists(path, model_file):
    raise Exception("Model pickle already exists! Aborting...")
# Save the models
pickle.dump(list_of_models, open(path+model_file, "wb"))
print("Saved list of models!", flush=True)

if file_exists(path, explanations_file):
    raise Exception("Explanation pickle already exists! Aborting...")
# Save the list of explanations
pickle.dump(list_of_explanations, open(path+explanations_file, "wb"))
print("Saved list of explanations!", flush=True)

if file_exists(path, intersection_explanations_file):
    raise Exception("Intersection explanation pickle already exists! Aborting...")
# Save the list of explanation intersections
pickle.dump(malw_to_inters, open(path+intersection_explanations_file, "wb"))
print("Saved intersection of explanations!", flush=True)

if file_exists(path, class_explanations_file):
    raise Exception("Class explanation pickle already exists! Aborting...")
# Save the list of explanation intersections per class
pickle.dump(class_to_malware_to_explanation, open(path+class_explanations_file, "wb"))
print("Saved class explanations!", flush=True)

if file_exists(path, class_explanations_occurrence_file):
    raise Exception("Class explanation occurrence pickle already exists! Aborting...")
# Save the list of explanation intersections sorted by decreasing occurrence per class
pickle.dump(class_to_explanation_occ, open(path+class_explanations_occurrence_file, "wb"))
print("Saved class explanations occurrences!", flush=True)

if file_exists(path, parameters_file):
    raise Exception("Parameters pickle already exists! Aborting...")
# Save the list of parameters
pickle.dump(parameters, open(path+parameters_file, "wb"))
print("Saved parameters!", flush=True)

print("Done...", flush=True)

# %%

